<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Dinesh's Journal - LLM</title><link href="https://www.mdinesh.com/" rel="alternate"></link><link href="https://www.mdinesh.com/feeds/llm.atom.xml" rel="self"></link><id>https://www.mdinesh.com/</id><updated>2024-05-10T00:00:00+05:30</updated><entry><title>Compare Vector Databases for LLM applications</title><link href="https://www.mdinesh.com/vector-database-comparison-llms.html" rel="alternate"></link><published>2024-05-10T00:00:00+05:30</published><updated>2018-05-10T00:00:00+05:30</updated><author><name>Dinesh</name></author><id>tag:www.mdinesh.com,2024-05-10:/vector-database-comparison-llms.html</id><summary type="html">&lt;p&gt;Comparison of various vector databases&lt;/p&gt;</summary><content type="html">&lt;style&gt;
  table {
    border-collapse:collapse; 
    table-layout:fixed; 
    width:310px;
    font-size: 11px;
  }
  table td {
    border:solid 1px #fab; 
    width:100px; 
    word-wrap:break-word;
  }
  &lt;/style&gt;

&lt;p&gt;We know the increasing importance of handling high-dimensional data, particularly NLP, and LLM applications. &lt;/p&gt;
&lt;p&gt;I have recently spent considerable amount of time: researching and comparious various vector databases for our Gen AI based projects. &lt;/p&gt;
&lt;h4&gt;Understanding the Need for Vector Databases:&lt;/h4&gt;
&lt;p&gt;I think it is crucial to understand the why vector databases are gaining traction ofver RDBMS or NoSQL for specific purposes.&lt;/p&gt;
&lt;p&gt;Unlike traditional databases that store structured data in rows, and columns, Vector Databases are excellent at storing and search high dimensional data in the form of vectors.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;p&gt;[0.81, 0.19, 0.58, 0.99, .........]&lt;/p&gt;
&lt;p&gt;This is how Indexing in Vector DB works. &lt;/p&gt;
&lt;!-- ![without_lazy_loading](images/../../images/llm_vectordb_comparison/vector_db_how_it_works.jpg) --&gt;
&lt;p&gt;&lt;img src="images/../../images/llm_vectordb_comparison/vector_db_indexing_how_it_works.jpg" alt="land cover image" style="width: 600px; height: 450px;"/&gt;&lt;/p&gt;
&lt;p&gt;This is how querying on top of Vector DB works.&lt;/p&gt;
&lt;p&gt;&lt;img src="images/../../images/llm_vectordb_comparison/vector_db_querying_how_it_works.jpg" alt="land cover image" style="width: 600px; height: 450px;"/&gt;&lt;/p&gt;
&lt;p&gt;This vector representation which we often call as embeddings, is required because AI/ML models understand numbers and we need vector form to train the models.
This capability is essential for tasks like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Similarity Search: Finding items similar to a given item based on their vector representation&lt;/li&gt;
&lt;li&gt;Question Answering: Answering User queries in a conversational manner.&lt;/li&gt;
&lt;li&gt;Semantic Search: Understand the underlying meaning of intent of a search keyword, and deliver accurate results.&lt;/li&gt;
&lt;li&gt;Document Classification: Classify a document into binary or multi class.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;to name a few.&lt;/p&gt;
&lt;h4&gt;Why use Vector DB over RDBMS for Embedding based apps:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Vector DB are collections with high dimensional vectors where as RDBMS has a table kind of structure with rows &amp;amp; columns&lt;/li&gt;
&lt;li&gt;Vector DBs have Unique Identifiers(ID) with less restrictions which makes it easy to build applications on top of it, where as RDBMS has complex Primary and Foreign keys&lt;/li&gt;
&lt;li&gt;Vector DBs use special indexes(such as inverted, HNSW) which makes is suitable for LLM apps, where as B-tree in the case of RDBMS.&lt;/li&gt;
&lt;li&gt;APIs are available to CRUD operations such as insert, search, update, upsert, delete which is very easy to use compared SQL in RDBMS.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Spent a lot of time to figure out which Vector DB to use and noted down the points below. 
Depending on your usecase, please choose the right one. &lt;/p&gt;
&lt;p&gt;For example, if you want a verticaly scalable, low latency DB, you can choose Pinecone or Qdrant. If you want to do vector search using SQL, choose PGVector. &lt;/p&gt;
&lt;table&gt;
    &lt;tr&gt;
        &lt;td&gt;Vector DB&lt;/td&gt;
        &lt;td&gt;When to Use&lt;/td&gt;
        &lt;td&gt;Scalability&lt;/td&gt;
        &lt;td&gt;OpenSource/Enterprise&lt;/td&gt;
        &lt;td&gt;Functionality&lt;/td&gt;
        &lt;td&gt;Performance&lt;/td&gt;
        &lt;td&gt;Type&lt;/td&gt;
        &lt;td&gt;Suitable &lt;br&gt;For&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;FAISS&lt;/td&gt;
        &lt;td&gt;Efficient for Large datasets&lt;/td&gt;
        &lt;td&gt;NA&lt;/td&gt;
        &lt;td&gt;Opensource&lt;/td&gt;
        &lt;td&gt;GPU availability&lt;/td&gt;
        &lt;td&gt;?&lt;/td&gt;
        &lt;td&gt;Vector Search&lt;/td&gt;
        &lt;td&gt;Offline evaluations/POCs.&lt;br&gt;Not suitable for Production&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Milvus&lt;/td&gt;
        &lt;td&gt;1. Supports massive vector search&lt;br&gt;2. Auto Index Management&lt;br&gt;3. GPU availability&lt;/td&gt;
        &lt;td&gt;Horizontal&lt;/td&gt;
        &lt;td&gt;Opensource&lt;/td&gt;
        &lt;td&gt;11 Index,&lt;br&gt;Multi Vector Query,&lt;br&gt;Attribute Filtering,&lt;br&gt;GPU, ..&lt;/td&gt;
        &lt;td&gt;Low latency&lt;/td&gt;
        &lt;td&gt;Purpose built Vector DB&lt;/td&gt;
        &lt;td&gt;Large Data Volume&lt;br&gt;and Scalable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Chroma&lt;/td&gt;
        &lt;td&gt;Multimodal data&lt;/td&gt;
        &lt;td&gt;NA&lt;/td&gt;
        &lt;td&gt;Opensource&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;?&lt;/td&gt;
        &lt;td&gt;Lightweight Vector DB&lt;/td&gt;
        &lt;td&gt;Not suitable for&lt;br&gt;Production&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ElasticSearch&lt;/td&gt;
        &lt;td&gt;Mature, Full-text Vector Search&lt;/td&gt;
        &lt;td&gt;Horizontal&lt;/td&gt;
        &lt;td&gt;Commercial&lt;/td&gt;
        &lt;td&gt;High Flexibility,&lt;br&gt;Query Filtering&lt;/td&gt;
        &lt;td&gt;High Latency&lt;/td&gt;
        &lt;td&gt;Vector Search Plugins&lt;/td&gt;
        &lt;td&gt;Suitable for Production.&lt;br&gt;Flexible queries can help to improve&lt;br&gt;efficiency&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;PgVector&lt;/td&gt;
        &lt;td&gt;Vector Search using SQL&lt;/td&gt;
        &lt;td&gt;NA&lt;/td&gt;
        &lt;td&gt;Opensource&lt;br&gt;Azure pgvector&lt;br&gt;(enterprise)&lt;br&gt;GCP pgvector&lt;br&gt;(enterprise)&lt;/td&gt;
        &lt;td&gt;Supports one index only&lt;/td&gt;
        &lt;td&gt;Low latency&lt;/td&gt;
        &lt;td&gt;Postgres Extension&lt;/td&gt;
        &lt;td&gt;Efficient but depends on Postgres.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;PineCone DB&lt;/td&gt;
        &lt;td&gt;Scalable, Instant Indexing&lt;/td&gt;
        &lt;td&gt;Vertical&lt;/td&gt;
        &lt;td&gt;Enterprise&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;Low latency&lt;/td&gt;
        &lt;td&gt;Enterprise cloud-native&lt;br&gt;Vector database&lt;/td&gt;
        &lt;td&gt;Production ready and Efficient&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Qdrant&lt;/td&gt;
        &lt;td&gt;Filtering, precise matching&lt;/td&gt;
        &lt;td&gt;Vertical&lt;/td&gt;
        &lt;td&gt;Opensource&lt;/td&gt;
        &lt;td&gt;Additional payloads&lt;br&gt;to filter results&lt;/td&gt;
        &lt;td&gt;Low latency&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;Moderate data volume and Scalable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;LanceDB&lt;/td&gt;
        &lt;td&gt;High Performance, Real time&lt;/td&gt;
        &lt;td&gt;NA&lt;/td&gt;
        &lt;td&gt;Opensource&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;td&gt;Lightweight Vector DB&lt;/td&gt;
        &lt;td&gt;Production ready and Efficient&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;Weaviate&lt;/td&gt;
        &lt;td&gt;Knowledge Graph, GraphQL&lt;/td&gt;
        &lt;td&gt;Horizontal&lt;/td&gt;
        &lt;td&gt;Opensource&lt;/td&gt;
        &lt;td&gt;Hybrid Search&lt;/td&gt;
        &lt;td&gt;Low latency&lt;/td&gt;
        &lt;td&gt;Lightweight Vector DB&lt;/td&gt;
        &lt;td&gt;Moderate data volume and Scalable&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;h3&gt;Example code to index documents in Pinecone (Enterprise DB)&lt;/h3&gt;
&lt;p&gt;In the code below, we are using sentence splitter to split the documents into chunks, and applying embedding model(example OpenAI, Gemini etc) to extract the embeddings in the function create_ingestion_pipeline&lt;/p&gt;
&lt;p&gt;and we are indexing the embeddings in Pinecone DB in the function store_embeddings_in_pinecone&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;create_ingestion_pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vector_store&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;sentence_splitter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SentenceSplitter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chunk_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1024&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chunk_overlap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pipeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;IngestionPipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;transformations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="n"&gt;sentence_splitter&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
            &lt;span class="n"&gt;embed_model&lt;/span&gt;
            &lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;vector_store&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;vector_store&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;docstore&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SimpleDocumentStore&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="c1"&gt;# For Document Management | Avoiding duplicates in index&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;store_embeddings_in_pinecone&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Extract Embeddings(defined in IngestionPipeline) and store in PineconeDB&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;        documents ([type]): [description]&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;pinecone_client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Pinecone&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;api_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pinecone_key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pinecone_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pinecone_client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;demo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;vector_store&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PineconeVectorStore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pinecone_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pinecone_index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pipeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_ingestion_pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vector_store&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;documents&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;vector_store&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="LLM"></category><category term="Vector DB"></category><category term="LLM"></category></entry><entry><title>Finetune Pretrained LLM on your custom data with OpenAI and LLAMA Index</title><link href="https://www.mdinesh.com/train-pretrained-llm-on-custom-data.html" rel="alternate"></link><published>2023-04-05T00:00:00+05:30</published><updated>2023-04-05T00:00:00+05:30</updated><author><name>Dinesh</name></author><id>tag:www.mdinesh.com,2023-04-05:/train-pretrained-llm-on-custom-data.html</id><summary type="html">&lt;p&gt;How to train LLM on your custom data&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you have used OpenAI and would like to finetune the model on your own data.&lt;/p&gt;
&lt;p&gt;Let us setup the environment first&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pip3&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;openai&lt;/span&gt;
&lt;span class="n"&gt;pip3&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;llama&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt; 
&lt;span class="n"&gt;pip3&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;pypdf&lt;/span&gt; 
&lt;span class="n"&gt;pip3&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;gradio&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To train my custom document, in this case, I have downloaded the documment "Getting ROIC right" from EY website, and posted a few questions:&lt;/p&gt;
&lt;p&gt;Q1: Why is Return on Invested Capital Important?&lt;/p&gt;
&lt;p&gt;&lt;img alt="Question_1" src="images/../../images/custom_llamaindex_trained_model/Question_Why_is_Return_on_invested_Capital_important.png"&gt;&lt;/p&gt;
&lt;p&gt;Q2: Can you explain what a liability is? Can you give me a full list in bullet points?&lt;/p&gt;
&lt;p&gt;&lt;img alt="Question_2" src="images/../../images/custom_llamaindex_trained_model/Question_What_are_liabilities.png"&gt;&lt;/p&gt;
&lt;p&gt;Q3: Your role is investor and your task is to calculate ROIC. Also give me an example with actual numbers.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Question_2" src="images/../../images/custom_llamaindex_trained_model/Question_with_proper_prompt_calculate_ROIC.png"&gt;&lt;/p&gt;
&lt;p&gt;Now, lets talk about the code.&lt;/p&gt;
&lt;p&gt;My folder structure looks like this, and I have copied my pdf in training-data folder&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;training-data
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Folder_Structure" src="images/../../images/custom_llamaindex_trained_model/folder_structure.png"&gt;&lt;/p&gt;
&lt;p&gt;In the above image, I have my code in custom_chat.py. There are two other folders created by llama-index, the indexes folder that it built using your pdf content and flagged folder created by gradio.&lt;/p&gt;
&lt;p&gt;Now the code part:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;llama_index&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GPTVectorStoreIndex&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SimpleDirectoryReader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LLMPredictor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ServiceContext&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;StorageContext&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;load_index_from_storage&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;langchain&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OpenAI&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;gradio&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;OPENAI_API_KEY&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;INSERT_KEY_HERE&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;construct_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;directory_path&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# set number of output tokens&lt;/span&gt;
    &lt;span class="n"&gt;num_outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;

    &lt;span class="n"&gt;_llm_predictor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LLMPredictor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;llm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;OpenAI&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;temperature&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gpt-3.5-turbo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_tokens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_outputs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;service_context&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ServiceContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_defaults&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;llm_predictor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;_llm_predictor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;docs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleDirectoryReader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;directory_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GPTVectorStoreIndex&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_documents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;service_context&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;service_context&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;#Directory in which the indexes will be stored&lt;/span&gt;
    &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;storage_context&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;persist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;persist_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;indexes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;chatbot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="c1"&gt;# rebuild storage context&lt;/span&gt;
    &lt;span class="n"&gt;storage_context&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StorageContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_defaults&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;persist_dir&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;indexes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;#load indexes from directory using storage_context &lt;/span&gt;
    &lt;span class="n"&gt;query_engine&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_index_from_storage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;storage_context&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_query_engine&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;query_engine&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;#returning the response&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;

&lt;span class="c1"&gt;#Creating the web UIusing gradio&lt;/span&gt;
&lt;span class="n"&gt;iface&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gradio&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Interface&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;chatbot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;gradio&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Textbox&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Enter your question here&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                     &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;text&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Custom-trained AI Chatbot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#Constructing indexes based on the documents in traininData folder&lt;/span&gt;
&lt;span class="c1"&gt;#This can be skipped if you have already trained your app and need to re-run it&lt;/span&gt;
&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;construct_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;training-data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#launching the web UI using gradio&lt;/span&gt;
&lt;span class="n"&gt;iface&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;launch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;share&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="LLM"></category><category term="LLM"></category></entry></feed>