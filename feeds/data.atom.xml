<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Dinesh's Journal - Data</title><link href="https://www.mdinesh.com/" rel="alternate"></link><link href="https://www.mdinesh.com/feeds/data.atom.xml" rel="self"></link><id>https://www.mdinesh.com/</id><updated>2017-03-26T00:00:00+05:30</updated><entry><title>Benchmarking / Benchmarking various Data file formats - csv, h5, pytables(hdf5), npy, npz, joblib</title><link href="https://www.mdinesh.com/benchmarking-data-file-formats.html" rel="alternate"></link><published>2017-03-26T00:00:00+05:30</published><updated>2017-03-26T00:00:00+05:30</updated><author><name>Dinesh</name></author><id>tag:www.mdinesh.com,2017-03-26:/benchmarking-data-file-formats.html</id><summary type="html">&lt;p&gt;Benchmark various data file formats&lt;/p&gt;</summary><content type="html">&lt;p&gt;The data processing/feature engineering part is very important and time taking process while developing machine learning models. I usually have several different formats of data(after data processing and feature engineering) for ex: one data file may contain only normalized data, one file containing standardized data, various data files with varying features after performing feature selection. I then apply appropriate machine learning models to each of the file and see the cross validation results. &lt;/p&gt;
&lt;p&gt;Also it is important to note that when we save a file to disk and load it back again, serialization and de-serialization happens which can impact the time.&lt;/p&gt;
&lt;p&gt;So, the time to save the data to the disk, load the file back from the disk, memory used are very important factors that can save a lot of time for us.&lt;/p&gt;
&lt;h3&gt;Data Formats&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;csv&lt;/li&gt;
&lt;li&gt;h5&lt;/li&gt;
&lt;li&gt;pytables(hdf5)&lt;/li&gt;
&lt;li&gt;npy&lt;/li&gt;
&lt;li&gt;npz&lt;/li&gt;
&lt;li&gt;joblib&lt;/li&gt;
&lt;li&gt;Todo: hickle.&lt;/li&gt;
&lt;li&gt;Dropped: pickle(because pickle cannot handle larger data sizes. Based on my experiments pickle format has tipping point around 2 GB).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Array Sizes&lt;/h3&gt;
&lt;p&gt;Instead of benchmarking just one size, I used asymptotic analysis approach wherein you measure the size and time of various array sizes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;100 x 100&lt;/li&gt;
&lt;li&gt;1000 x 1000&lt;/li&gt;
&lt;li&gt;10000 x 10000&lt;/li&gt;
&lt;li&gt;20000 x 20000&lt;/li&gt;
&lt;li&gt;30000 x 30000&lt;/li&gt;
&lt;li&gt;40000 x 40000&lt;/li&gt;
&lt;li&gt;50000 x 50000&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;In Memory Sizes&lt;/h3&gt;
&lt;p&gt;The following table shows the size of in memory size consumed after creation of each array:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Array Size  -&lt;/th&gt;
&lt;th style="text-align: left;"&gt;In Memory(MB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;100 x 100   -&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.08&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;1000 x 1000 -&lt;/td&gt;
&lt;td style="text-align: left;"&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;10000 x 10000 -&lt;/td&gt;
&lt;td style="text-align: left;"&gt;800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;20000 x 20000 -&lt;/td&gt;
&lt;td style="text-align: left;"&gt;3200&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;30000 x 30000 -&lt;/td&gt;
&lt;td style="text-align: left;"&gt;7200&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;40000 x 40000 -&lt;/td&gt;
&lt;td style="text-align: left;"&gt;12800&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;50000 x 50000 -&lt;/td&gt;
&lt;td style="text-align: left;"&gt;20000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Lets see the same data visually:&lt;/p&gt;
&lt;!-- &lt;img src="images/benchmarking/inmemorysizes.png" alt="In memory sizes of various array sizes" style="width: 500px; height: 300px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="benchmarking_data_file_formats" src="images/../../images/benchmarking_data_formats/inmemorysizes.png"&gt;&lt;/p&gt;
&lt;h3&gt;Size on disk (in MB):&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CSV format consumed the largest size on disk when compared to other data formats.&lt;/li&gt;
&lt;li&gt;CSV format consumes 3x the size of all other formats.&lt;/li&gt;
&lt;li&gt;We can see from the below chart that size of the csv file increases exponentially and way far away from other file formats.&lt;/li&gt;
&lt;li&gt;Surprising point is, except csv, all other file formats consume the same disk space as the memory size.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- &lt;img src="images/benchmarking/size_on_disk.png" alt="Size on disk" style="width: 500px; height: 300px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="size_on_disk" src="images/../../images/benchmarking_data_formats/size_on_disk.png"&gt;&lt;/p&gt;
&lt;h3&gt;Time to save files to disk (in minutes):&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Overall, h5 and hdf5 take less time to save followed by joblib and npy and then npz.&lt;/li&gt;
&lt;li&gt;CSV again performed the worst.&lt;/li&gt;
&lt;li&gt;On an average, CSV consumes 70x times more than the least time taken by any other file format.&lt;/li&gt;
&lt;li&gt;As you can see from the below visual, CSV performs the worst.&lt;/li&gt;
&lt;li&gt;While other formats take less than a minute to save 50000 x 50000 array, csv takes approximately 23 minutes.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- &lt;img src="images/benchmarking/time_to_disk.png" alt="Time to save to disk" style="width: 500px; height: 300px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="time_to_disk" src="images/../../images/benchmarking_data_formats/time_to_disk.png"&gt;&lt;/p&gt;
&lt;h3&gt;Time to load files from disk (in minutes):&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Overall npy format performs the best.&lt;/li&gt;
&lt;li&gt;CSV performs the worst.&lt;/li&gt;
&lt;li&gt;On an average, CSV consumes 140x times more than npy format to load the data from the disk.&lt;/li&gt;
&lt;li&gt;As you can see from the below visual, time taken to load the csv across various array sizes increases exponentially where as the other file formats show a linear pattern.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- &lt;img src="images/benchmarking/load_from_disk.png" alt="Time to save to disk" style="width: 500px; height: 300px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="load_from_disk" src="images/../../images/benchmarking_data_formats/load_from_disk.png"&gt;&lt;/p&gt;
&lt;h3&gt;Strange results:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;During my experiments, I used memory_profiler package to check the memory size after loading each file format. I noticed strange results, sometimes the size in memory of the largest array &amp;lt; smaller array sizes. I have to investigate this part.&lt;/li&gt;
&lt;/ul&gt;</content><category term="Data"></category><category term="Data Engineering"></category><category term="Benchmarking"></category></entry></feed>