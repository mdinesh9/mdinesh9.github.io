<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Dinesh's Journal - Machine Learning</title><link href="https://www.mdinesh.com/" rel="alternate"></link><link href="https://www.mdinesh.com/feeds/machine-learning.atom.xml" rel="self"></link><id>https://www.mdinesh.com/</id><updated>2017-10-10T13:34:00+05:30</updated><entry><title>Machine Learning / Predict the Customer segment in Advertising - Top 1% in IndiaHacks Machine Learning Competition</title><link href="https://www.mdinesh.com/customer-segmentation-advertising-indiahacks.html" rel="alternate"></link><published>2017-10-10T13:34:00+05:30</published><updated>2016-10-02T21:27:00+05:30</updated><author><name>Dinesh</name></author><id>tag:www.mdinesh.com,2017-10-10:/customer-segmentation-advertising-indiahacks.html</id><summary type="html">&lt;p&gt;Solution to predicting the customer segmentation in Advertising industry.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Indiahacks Machine Learning competition is an All India machine learning competition conducted once in a year. I participated in the qualification round and secured 6th position(out of 6000 participants), which is Top 1%. Only top 60 participants were selected to participate in offline zonal round. However, I was unable to participate in zonal round since I was traveling.&lt;/p&gt;
&lt;p&gt;Note: Code is not production ready yet, so not sharing it on github. Will share it when I get some free time.&lt;/p&gt;
&lt;p&gt;The challenge was to predict the segment(pos, neg) based on the given features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ID: unique identifier variable&lt;/li&gt;
&lt;li&gt;titles: format “title:watch_time”, titles of the shows watched by the user and watch_time on different titles&lt;/li&gt;
&lt;li&gt;genres: same format as titles&lt;/li&gt;
&lt;li&gt;cities: same format as titles&lt;/li&gt;
&lt;li&gt;tod: total watch time of the user spread across different time of days (24 hours format)&lt;/li&gt;
&lt;li&gt;dow: total watch time of the user spread across different days of week (7 days format)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Model Pipeline:&lt;/h3&gt;

&lt;p&gt;&lt;img src="images/indiahacks/segment_hotstar_architecture.png" alt="Segment Hotstar Competition Pipeline" style="width: 550px; height: 400px;"/&gt;&lt;/p&gt;
&lt;h3&gt;Features extracted from text variables:&lt;/h3&gt;

&lt;p&gt;Titles variable: I used word embedding using word2vec, a deep learning technique which maps similar words to context after trying Bag of Words. Word embedding improved my validation score significantly.&lt;/p&gt;
&lt;p&gt;Tod variable: Several features were extracted from total watch time column out of which I ended up using the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tod_median_time&lt;/li&gt;
&lt;li&gt;tod_min_time&lt;/li&gt;
&lt;li&gt;dow_max_time&lt;/li&gt;
&lt;li&gt;watch time counts at hours (0 to 23) mapped from t0 to t23&lt;/li&gt;
&lt;li&gt;tod_start&lt;/li&gt;
&lt;li&gt;tod_end&lt;/li&gt;
&lt;li&gt;Days&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cities variable: Several features were extracted out of which I ended up using the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cities_min_time&lt;/li&gt;
&lt;li&gt;cities_count&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Genres variable: Extracted the genres from this column and mapped each genre as a binary feature.&lt;/p&gt;
&lt;p&gt;Apart from these, I extracted the following features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;genres_min_time&lt;/li&gt;
&lt;li&gt;genres_max_time&lt;/li&gt;
&lt;li&gt;genres_mean_time&lt;/li&gt;
&lt;li&gt;genres_median_time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other text related features which improved by validation score are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;titles length&lt;/li&gt;
&lt;li&gt;titles count&lt;/li&gt;
&lt;li&gt;cities strings&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Features extracted from numeric columns:&lt;/h3&gt;

&lt;p&gt;dow variable:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Binary features for each day of the week starting from Monday to Sunday.&lt;/li&gt;
&lt;li&gt;Watch time spent on each day from Monday to Sunday.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Model Evaluation:&lt;/h3&gt;

&lt;p&gt;I tried several different models which produced the following scores:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Model&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Logistic Regression&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Linear Discriminant Analysis&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;K Nearest Neighbors&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.59&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;CART&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;AdaBoost&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Gradient Boosting&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Random Forests&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Extra Trees Regressor&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;LightGBM(after hyper parameter tuning)&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.822&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Xgboost(after hyper parameter tuning)&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.821&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I ended up using LightGBM to generate my predictions.&lt;/p&gt;
&lt;h3&gt;Hyper parameter tuning:&lt;/h3&gt;
&lt;p&gt;Used hyperopt to automatically find the right hyper parameters which improved my validation score for LightGBM model&lt;/p&gt;
&lt;p&gt;The competition rewarded contestants who did feature engineering.&lt;/p&gt;
&lt;h3&gt;Things that I tried which din't work:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Bag of Words approach.&lt;/li&gt;
&lt;li&gt;Dimensionality reduction on Word2vec features.&lt;/li&gt;
&lt;li&gt;Several extracted numerical and text features.&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category><category term="Machine Learning"></category><category term="Competitions"></category><category term="Natural Language Processing"></category></entry><entry><title>Machine Learning / Land Cover Classification with 96% F1 Score - European Conference of Machine Learning-PKDD</title><link href="https://www.mdinesh.com/land-cover-classification-european-conference-pkdd.html" rel="alternate"></link><published>2017-07-30T17:17:00+05:30</published><updated>2017-07-30T17:17:00+05:30</updated><author><name>Dinesh</name></author><id>tag:www.mdinesh.com,2017-07-30:/land-cover-classification-european-conference-pkdd.html</id><summary type="html">&lt;p&gt;Land Cover Classification with 96% Accuracy.&lt;/p&gt;</summary><content type="html">&lt;p&gt;European Conference of Machine Learning - Principles and Practice of Knowledge Discovery in Databases conducted a &lt;a href="https://sites.google.com/site/dinoienco/tiselc"&gt;Machine Learning competition&lt;/a&gt; where the task was to classify the land cover.&lt;/p&gt;
&lt;p&gt;Unfortunately, I was unable to submit my prediction data points on time. But I got 96.34 % accuracy which would have been &lt;span style="color:#FF4633"&gt;&lt;b&gt;4th position&lt;/b&gt;&lt;/span&gt; in the competition. Anyways I described my approach below.&lt;/p&gt;
&lt;p&gt;The classification of land cover was divided into the following multi-class(9 classes) distribution.&lt;/p&gt;
&lt;!-- &lt;img src="images/landcover_classification/land_cover_classes.png" alt="Classification table" style="width: 350px; height: 300px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="land_cover_classes" src="images/../../images/landcover_classification/land_cover_classes.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style="color:#04847E"&gt;Below picture depicts the class distribution.&lt;/span&gt;&lt;/p&gt;
&lt;!-- &lt;img src="images/landcover_classification/land_cover_photo.png" alt="land cover image" style="width: 450px; height: 300px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="land_cover_photo" src="images/../../images/landcover_classification/land_cover_photo.png"&gt;&lt;/p&gt;
&lt;h2&gt;My Approach:&lt;/h2&gt;

&lt;p&gt;There were 230 columns which contained -ve &amp;amp; +ve data points representing the land cover.&lt;/p&gt;
&lt;h3&gt;Outliers Removal:&lt;/h3&gt;
&lt;p&gt;All the columns had few rows with outliers which were removed.&lt;/p&gt;
&lt;p&gt;Boxplot depicting outliers in variable col1:&lt;/p&gt;
&lt;!-- &lt;img src="images/landcover_classification/outlier_sample_shot.png" alt="Classification table" style="width: 250px; height: 200px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="outlier_sample_shot" src="images/../../images/landcover_classification/outlier_sample_shot.png"&gt;&lt;/p&gt;
&lt;h3&gt;Feature Engineering:&lt;/h3&gt;
&lt;p&gt;I extracted several features out of which I ended up using the following features after feature selection.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;coord1_col_1_std - Standard deviation of col1 grouped by coord1.&lt;/li&gt;
&lt;li&gt;coord_diff_1 - coord1 minus coord2 variables.&lt;/li&gt;
&lt;li&gt;coord_diff_2 - coord2 minus coord1 variables.&lt;/li&gt;
&lt;li&gt;coords_combined - coord1 + coord2 variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall, I ended up using 13 features after feature selection.&lt;/p&gt;
&lt;h3&gt;BoxCox Transformation for Skewed Variables :&lt;/h3&gt;

&lt;p&gt;Most of the variables were highly skewed.&lt;/p&gt;
&lt;!-- &lt;img src="images/landcover_classification/skewed_variables.png" alt="Classification table" style="width: 250px; height: 200px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="skewed_variables" src="images/../../images/landcover_classification/skewed_variables.png"&gt;&lt;/p&gt;
&lt;!-- &lt;img src="images/landcover_classification/skewed_pictures.png" alt="Classification table" style="width: 450px; height: 400px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="skewed_pictures" src="images/../../images/landcover_classification/skewed_pictures.png"&gt;&lt;/p&gt;
&lt;p&gt;I applied box-cox transformation on variables with (+-)ve 0.25 skew.&lt;/p&gt;
&lt;!-- &lt;img src="images/landcover_classification/unskewed.png" alt="Classification table" style="width: 250px; height: 200px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="unskewed" src="images/../../images/landcover_classification/unskewed.png"&gt;&lt;/p&gt;
&lt;!-- &lt;img src="images/landcover_classification/unskewed_pictures.png" alt="Classification table" style="width: 450px; height: 400px;"/&gt; --&gt;

&lt;p&gt;&lt;img alt="unskewed_pictures" src="images/../../images/landcover_classification/unskewed_pictures.png"&gt;&lt;/p&gt;
&lt;h3&gt;Standardize data:&lt;/h3&gt;
&lt;p&gt;I applied Standard Scaling transformation to standardize the data.&lt;/p&gt;
&lt;h3&gt;Things that I tried which didn't improve Validation score: &lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Polynomial features/ Feature interactions.&lt;/li&gt;
&lt;li&gt;Mean, standard deviations, medians(Measures of central tendency) grouped by coordinates.&lt;/li&gt;
&lt;li&gt;Robust Scaling before removing the outliers.&lt;/li&gt;
&lt;li&gt;Stacking multiple models.&lt;/li&gt;
&lt;li&gt;Max voting based on multiple models.&lt;/li&gt;
&lt;li&gt;Dimensionality reduction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Model Scores:&lt;/h3&gt;

&lt;p&gt;I tried several models which resulted in the following &lt;b&gt;local&lt;/b&gt; validation scores:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Model&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Validation Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;XGboost(Boosting):&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Linear Regression:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.68&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Passive Aggressive Classifier:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.47&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;SGD Classifier:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Linear Discriminant Analysis:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.67&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;KNeighbors Classifier:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.88&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Decision Tree Classifier:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;GaussianNB:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;BernoulliNB:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.57&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;AdaBoost Classifier:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Gradient Boosting Classifier:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Random Forest Classifier:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;Extra Trees Classifier:&lt;/td&gt;
&lt;td style="text-align: left;"&gt;0.95&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Code available at &lt;a href="https://github.com/pushlog/tiselac_competition"&gt;github&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"></category><category term="Machine Learning"></category><category term="Competitions"></category></entry></feed>